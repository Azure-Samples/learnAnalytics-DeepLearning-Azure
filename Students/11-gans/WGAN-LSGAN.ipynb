{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN and Loss Sensitive GAN with CIFAR Data\n",
    "\n",
    "**Prerequisites: ** Please run the notebook _CIFAR-10_DataLoader.ipynb_ prior to this notebook. Be patient, it'll take 10-15 minutes to run!\n",
    "\n",
    "## Introduction\n",
    "Generative models have gained a lot of attention in deep learning community which has traditionally leveraged discriminative  models for semi-supervised and unsupervised learning. [Generative Adversarial Network (GAN)](https://arxiv.org/pdf/1406.2661v1.pdf) (Goodfellow *et al.*, 2014) is one of the most popular generative model because of its promising results in [various tasks](https://github.com/HKCaesar/really-awesome-gan) in computer vision and natural language processing. However, the original version of GANs are notorious for being difficult to train. Without carefully-chosen hyper-parameters and network architecture that balances Generator and Discriminator training, GANs could easily suffer from vanishing gradient or mode collapse (where the model is only able to produce a single or a few samples). In this tutorial, we introduce several improved GAN models, namely [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) (W-GAN) (Arjovsky *et al.*, 2017) and [Loss Sensitive GAN](https://arxiv.org/pdf/1701.06264.pdf) (LS-GAN) (Qi, 2017), that are proposed to address the problems of vanishing gradient and mode collapse.\n",
    "\n",
    "\n",
    "## Overview\n",
    "In this section, we will briefly overview of main differences between Wasserstein GANs and original GANs in both theory and implementation perspective.\n",
    "\n",
    "### Why is GAN hard to train?\n",
    "_**[TL;DR]** In the training of the original GANs, balancing the convergence of the discriminator and the generator is extremely important because if one is far ahead of the other, the other can not get enough gradient to improve. However, balancing the convergence of two neural networks is hard._\n",
    "\n",
    "If you are interested in the math behind it, you can take a look at the rest of this part and [this paper here](https://arxiv.org/pdf/1701.04862.pdf). If not, you can just skip the math and look at the implementation details for W-GAN and LS-GAN.\n",
    "\n",
    "In [the original GAN paper](https://arxiv.org/pdf/1406.2661v1.pdf), GAN includes two neural network, a Generator $G$ and a Discriminator $D$. The training of GAN is modeled as a two-player zeros-sum game. The Discriminator D is trained to predict the probability that a sample is a real sample rather than generated from the generator G, while the generator G is trained to better fool the discriminator by producing real-looking samples. The objective for GAN training is,\n",
    "\n",
    "$$\\min_G\\max_D V(D,G)=\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(x)}[\\log(1-D(G(z)))]$$\n",
    "\n",
    "In the original GAN paper, the author proves that the optimal strategy for discriminator is predicting\n",
    "\n",
    "$$D^*(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_{model}(x)}$$\n",
    "\n",
    "By plugging it into the GAN objective function, one may find that the discriminator is actually an estimation of *Jensen-Shannon* divergence (JS divergence or JSD) of two distributions (data and model).\n",
    "\n",
    "$$L(D^*,g_\\theta)=2 JSD(\\mathbb{P}_{data}\\|\\mathbb{P}_{model}) - 2\\log2$$\n",
    "\n",
    "JS distance may become locally saturated and gets vanishing gradient to train the GAN generator if the discriminator is over-trained.\n",
    "\n",
    "### Wasserstein GAN\n",
    "To address this problem, [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf) was proposed to use a different distance measurement for probability distributions, namely *Earth-Mover* (EM) distance or *Wasserstein* distance instead of JS divergence. The authors claimed that by using EM distance, one no longer needs to carefully maintain the balance between the generator and the discriminator, and, notably, the output of the discriminator (they call it critic instead in the paper), which is an estimation of EM distance serves as a good indicator of image quality of generated samples. The EM distance of two distribution is defined as\n",
    "\n",
    "$$W(p_{data}, p_{model})=\\inf_{\\gamma\\in\\prod(p_{data},p_{model})}\\mathbb{E}_{(x,y)\\sim\\gamma}\\left[\\|x-y\\|\\right]$$\n",
    "\n",
    "In the paper shows that EM distance is a more sensible distance measurement than JS divergence since EM distance is continuous and differentiable anywhere while JS divergence is not. The authors uses the  Kantorovich-Rubinstein duality to derive the objective for Wasserstein GAN,\n",
    "\n",
    "$$\\min_G\\max_{\\|D\\|_L\\leq K} \\mathbb{E}_{x\\sim p_{data}(x)}[D(x)] - \\mathbb{E}_{z\\sim p_z(x)}[D(G(z))]$$\n",
    "\n",
    "**Note: **the Kantorovich-Rubinstein duality requires the function to be K-Lipschitz. The authors suggests *clipping the weights of discriminator* to satisfy Lipschitz continuity.\n",
    "\n",
    "#### Implementation details\n",
    "\n",
    "The modification needed on implementation side is minor. On can change an original GAN into a Wasserstein GAN with a few lines of code:\n",
    "\n",
    "1. Use W-GAN loss function\n",
    "2. Remove the sigmoid activation for the last layer of discriminator\n",
    "3. Clip the weights of the discriminator after updates (e.g., to [-0.01, 0.01])\n",
    "4. Train discriminator more iterations than generator (e.g., train the discriminator for 5 iterations and train the generator for one iteration only at each round)\n",
    "5. Use non-momentum-based optimizer (e.g., RMSProp) instead of Adam (Note: in this tutorial we use Adam with `momentum=0`)\n",
    "6. Use small learning rate (e.g., 0.00005)\n",
    "\n",
    "### Loss Sensitive GAN\n",
    "\n",
    "[Loss Sensitive GAN](https://arxiv.org/pdf/1701.06264.pdf) was proposed to address the problem of vanishing gradient. LS-GAN is trained on a loss function that allows the generator to focus on improving poor generated samples that are far from the real sample manifold. The author shows that the loss learned by LS-GAN has non-vanishing gradient almost everywhere, even when the discriminator is over-trained.\n",
    "\n",
    "$$\\min_D L_D = \\mathbb{E}_{x\\sim p_{data}(x)}[D(x)] + \\lambda\\mathbb{E}_{x\\sim p_{data}(x), z\\sim p_z(x)}\\left[\\left(\\|x-G(z)\\|_1 + D(x) - D(G(z))\\right)_+\\right]$$\n",
    "$$\\min_G L_G = \\mathbb{E}_{z\\sim p_z(x)}[D(G(z))]$$\n",
    "\n",
    "#### Implementation details\n",
    "\n",
    "The modification needed on implementation side is also minor. On can change an original GAN into a Wasserstein GAN with a few lines of code:\n",
    "\n",
    "1. Use the LS-GAN loss function\n",
    "2. Remove the sigmoid activation for the last layer of discriminator\n",
    "3. Update both the generator and the discriminator with weight decay\n",
    "4. Train discriminator and generator each with one iteration at each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two run modes:\n",
    "* *Fast mode: * `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n",
    "* *Slow mode: * We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training.\n",
    "\n",
    "**Note: **If the `isFlag` is set to `False` the notebook will take a hours or even days on a GPU enabled machine. You can try fewer iterations by setting the `num_minibatches` to a smaller number which comes at the expense of quality of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isFast = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading\n",
    "The input to the GANs will be a vector of random numbers. At the end of the training, the GAN \"learns\" to generate images drawn from the CIFAR dataset. We will be using the same CIFAR data prepared in tutorial CNTK 201A. For our purposes, you only need to know that the following function returns an object that will be used to read images from the CIFAR dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image dimensionalities\n",
    "img_h, img_w = 32, 32\n",
    "img_c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the data path for testing\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): return envvar in os.environ\n",
    "\n",
    "if is_test():\n",
    "    data_path = os.path.join(os.environ[envvar],'Image','CIFAR','v0','tutorial201')\n",
    "    data_path = os.path.normpath(data_path)\n",
    "else:\n",
    "    data_path = os.path.join('data', 'CIFAR-10')\n",
    "    \n",
    "train_file = os.path.join(data_path, 'train_map.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(map_file, train):\n",
    "    print(\"Reading map file:\", map_file)\n",
    "    \n",
    "    if not os.path.exists(map_file):\n",
    "        raise RuntimeError(\"This tutorials depends 201A tutorials, please run 201A first.\")\n",
    "    \n",
    "    import cntk.io.transforms as xforms\n",
    "    transforms = [xforms.crop(crop_type='center', side_ratio=0.8),\n",
    "                  xforms.scale(width=img_w, height=img_h, channels=img_c, interpolations='linear')]\n",
    "    # deserializer\n",
    "    return C.io.MinibatchSource(C.io.ImageDeserializer(map_file, C.io.StreamDefs(\n",
    "        features = C.io.StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = C.io.StreamDef(field='label', shape=10)      # and second as 'label'\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noise_sample(num_samples):\n",
    "    return np.random.uniform(\n",
    "        low = -1.0,\n",
    "        high = 1.0,\n",
    "        size = [num_samples, g_input_dim]\n",
    "    ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation (W-GAN)\n",
    "\n",
    "Note that we assume that you have already completed the DCGAN tutorial. If you need a basic recap of GAN concepts or DCGAN architecture, please visit our [DCGAN tutorial](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_206B_DCGAN.ipynb). \n",
    "### Model Configuration\n",
    "We implemented the W-GAN based on [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) architecture. In this step, we establish some of the architectural and training hyper-parameters for our model.\n",
    "* The generator is fractional strided convolutional network with $5\\times5$ kernels and strides of $2$\n",
    "* The input of the generator is a 100-dimensional random vector\n",
    "* The output of the generator is a flattened $64\\times64$ image with $3$ channels\n",
    "* The discriminator is a strided convolutional network with $5\\times5$ kernels and strides of $2$\n",
    "* The input of the discriminator is also a flattened $64\\times64$ image with $3$ channels\n",
    "* The output of the discriminator is a scalar which is an estimation of EM distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# architectural hyper-parameters\n",
    "gkernel = dkernel = 5\n",
    "gstride = dstride = 2\n",
    "\n",
    "# Input / Output parameter of Generator and Discriminator\n",
    "g_input_dim = 100\n",
    "g_output_dim = d_input_dim = (img_c, img_h, img_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first establish some of the helper functions (batch normalization with relu and batch normalization with leaky relu) that will make our lives easier when defining the generator and the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def bn_with_relu(x, activation=C.relu):\n",
    "    h = C.layers.BatchNormalization(map_rank=1)(x)\n",
    "    return C.relu(h)\n",
    "\n",
    "# We use param-relu function to use a leak=0.2 since CNTK implementation \n",
    "# of Leaky ReLU is fixed to 0.01\n",
    "def bn_with_leaky_relu(x, leak=0.2):\n",
    "    h = C.layers.BatchNormalization(map_rank=1)(x)\n",
    "    r = C.param_relu(C.constant((np.ones(h.shape)*leak).astype(np.float32)), h)\n",
    "    return r\n",
    "\n",
    "def leaky_relu(x, leak=0.2):\n",
    "    return C.param_relu(C.constant((np.ones(x.shape)*leak).astype(np.float32)), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "We define the generator according to the DCGAN architecture. The generator takes a 100-dimensional random vector as input and outputs a flattened $3\\times64\\times64$ image. We use fractionally strided convolution layers with relu convolution and batch normalization except for the last layer, where we use tanh to normalize the output to the interval $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_generator(z):\n",
    "    with C.layers.default_options(init=C.normal(scale=0.02)):\n",
    "        \n",
    "        gfc_dim = 256\n",
    "        gf_dim = 64\n",
    "        \n",
    "        print('Generator input shape: ', z.shape)\n",
    "        \n",
    "        h0 = C.layers.Dense([gfc_dim, img_h//8, img_w//8], activation=None)(z)\n",
    "        h0 = bn_with_relu(h0)\n",
    "        print('h0 shape', h0.shape)\n",
    "\n",
    "        h1 = C.layers.ConvolutionTranspose2D(gkernel,\n",
    "                                  num_filters=gf_dim*2,\n",
    "                                  strides=gstride,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h//4, img_w//4),\n",
    "                                  activation=None)(h0)\n",
    "        h1 = bn_with_relu(h1)\n",
    "        print('h1 shape', h1.shape)\n",
    "\n",
    "        h2 = C.layers.ConvolutionTranspose2D(gkernel,\n",
    "                                  num_filters=gf_dim,\n",
    "                                  strides=gstride,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h//2, img_w//2),\n",
    "                                  activation=None)(h1)\n",
    "        h2 = bn_with_relu(h2)\n",
    "        print('h2 shape :', h2.shape)\n",
    "        \n",
    "        h3 = C.layers.ConvolutionTranspose2D(gkernel,\n",
    "                                  num_filters=img_c,\n",
    "                                  strides=gstride,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h, img_w),\n",
    "                                  activation=C.tanh)(h2)\n",
    "        print('h3 shape :', h3.shape)\n",
    "\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "We define the discriminator according to the DCGAN architecture except for the last layer. The discriminator takes a flattened image as input and outputs a single scalar. We do not use any activation at the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_discriminator(x):\n",
    "    with C.layers.default_options(init=C.normal(scale=0.02)):\n",
    "        \n",
    "        dfc_dim = 256\n",
    "        df_dim = 64\n",
    "        \n",
    "        print('Discriminator convolution input shape', x.shape)\n",
    "\n",
    "        h0 = C.layers.Convolution2D(dkernel, df_dim, strides=dstride, pad=True)(x)\n",
    "        h0 = leaky_relu(h0, leak=0.2)\n",
    "        print('h0 shape :', h0.shape)\n",
    "\n",
    "        h1 = C.layers.Convolution2D(dkernel, df_dim*2, strides=dstride, pad=True)(h0)\n",
    "        h1 = bn_with_leaky_relu(h1, leak=0.2)\n",
    "        print('h1 shape :', h1.shape)\n",
    "\n",
    "        h2 = C.layers.Convolution2D(dkernel, dfc_dim, strides=dstride, pad=True)(h1)\n",
    "        h2 = bn_with_leaky_relu(h2, leak=0.2)\n",
    "        print('h2 shape :', h2.shape)\n",
    "\n",
    "        h3 = C.layers.Dense(1, activation=None)(h2)\n",
    "        print('h3 shape :', h3.shape)\n",
    "\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training config\n",
    "minibatch_size = 64\n",
    "num_minibatches = 500 if isFast else 20000\n",
    "lr = 0.00005 # small learning rates are preferred\n",
    "momentum = 0.0 # momentum is not suggested since it can make W-GANs unstable\n",
    "clip = 0.01 # the weight clipping parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph\n",
    "\n",
    "The discriminator must be used on both the real CIFAR images and fake images generated by the generator function. One way to represent this in the computational graph is to create a clone of the output of the discriminator function, but with substituted inputs. Setting `method=share` in the clone function ensures that both paths through the discriminator model use the same set of parameters\n",
    "\n",
    "We need to update the parameters for the generator and discriminator model separately using the gradients from different loss functions. We can get the parameters for a Function in the graph with the parameters attribute. However, when updating the model parameters, update only the parameters of the respective models while keeping the other parameters unchanged. In other words, when updating the generator we will update only the parameters of the  function while keeping the parameters of the  function fixed and vice versa.\n",
    "\n",
    "Because W-GAN needs to clip the weights of the discriminator before every updates in order to maintain K-Lipschitz continuity. We build a graph with clipped parameters stored in `clipped_D_params`. The suggested value of clipping threshold is 0.01.\n",
    "\n",
    "**Note: ** CNTK parameter learner uses sum of gradient within a minibatch by default instead of mean of gradient. To reproduce  results with the same hyper-parameter in the paper, we need to set `use_mean_gradient = True`, and `unit_gain = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_WGAN_graph(noise_shape, image_shape, generator, discriminator):\n",
    "    \n",
    "    input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "    Z = C.input_variable(noise_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real = C.input_variable(image_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real_scaled = (X_real - 127.5) / 127.5\n",
    "\n",
    "    # Create the model function for the generator and discriminator models\n",
    "    X_fake = generator(Z)\n",
    "    D_real = discriminator(X_real_scaled)\n",
    "    D_fake = D_real.clone(\n",
    "        method = 'share',\n",
    "        substitutions = {X_real_scaled.output: X_fake.output}\n",
    "    )\n",
    "    \n",
    "    clipped_D_params = [C.clip(p, -clip, clip) for p in D_real.parameters]\n",
    "    \n",
    "    G_loss = - D_fake\n",
    "    D_loss = - D_real + D_fake\n",
    "\n",
    "    G_learner = C.adam(\n",
    "            parameters = X_fake.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "            \n",
    "    D_learner = C.adam(\n",
    "            parameters = D_real.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "    \n",
    "    # Instantiate the trainers\n",
    "    G_trainer = C.Trainer(X_fake,\n",
    "                        (G_loss, None),\n",
    "                        G_learner)\n",
    "    D_trainer = C.Trainer(D_real,\n",
    "                        (D_loss, None),\n",
    "                        D_learner)\n",
    "\n",
    "    return X_real, X_fake, D_real, clipped_D_params, Z, G_trainer, D_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "The code for training the GAN closely follows Algorithm 1 in the W-GAN paper. Note that compared to original GANs, we train the discriminator many more times than the generator. The reason behind that is the output of the discriminator serves as an estimation of the EM distance. We want to train the discriminator until it can closely estimate the EM distance. In order to make sure that the discriminator has a sufficient good estimation at the very beginning of the training, we even train it for 100 iterations before train the generator (this is disabled in fast mode because this will significantly take longer time).\n",
    "\n",
    "[placeholder for WGAN algorithm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_WGAN(reader_train, generator, discriminator):\n",
    "    X_real, X_fake, D_real, clipped_D_params, Z, G_trainer, D_trainer = \\\n",
    "        build_WGAN_graph(g_input_dim, d_input_dim, generator, discriminator)\n",
    "    # print out loss for each model for upto 25 times\n",
    "    \n",
    "    print_frequency_mbsize = num_minibatches // 25\n",
    "    \n",
    "    print(\"First row is Generator loss, second row is Discriminator loss\")\n",
    "    pp_G = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    pp_D = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    \n",
    "    input_map = {X_real: reader_train.streams.features}\n",
    "\n",
    "    for training_step in range(num_minibatches):\n",
    "        # train the discriminator model for diter steps\n",
    "        if not isFast and (training_step < 25 or training_step % 500 == 0):\n",
    "            diter = 100\n",
    "        else:\n",
    "            diter = 5\n",
    "        for d_train_step in range(diter):\n",
    "            for parameter, clipped in zip(D_real.parameters, clipped_D_params):\n",
    "                C.assign(parameter, clipped).eval()\n",
    "            Z_data = noise_sample(minibatch_size)\n",
    "            X_data = reader_train.next_minibatch(minibatch_size, input_map)\n",
    "            batch_inputs = {X_real: X_data[X_real].data, Z: Z_data}\n",
    "            D_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        batch_inputs = {Z: Z_data}\n",
    "        G_trainer.train_minibatch(batch_inputs)\n",
    "       \n",
    "        pp_G.update_with_trainer(G_trainer)\n",
    "        pp_D.update_with_trainer(D_trainer)\n",
    "\n",
    "    G_trainer_loss = G_trainer.previous_minibatch_loss_average\n",
    "    return Z, X_fake, G_trainer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader_train = create_reader(train_file, True)\n",
    "\n",
    "# G_input, G_output, G_trainer_loss = train(reader_train, dense_generator, dense_discriminator)\n",
    "G_input, G_output, G_trainer_loss = train_WGAN(reader_train,\n",
    "                                          convolutional_generator,\n",
    "                                          convolutional_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the generator loss \n",
    "print(\"Training loss of the generator is: {0:.2f}\".format(G_trainer_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake (Synthetic) Images (W-GAN)\n",
    "Now that we have trained the model, we can create fake images simply by feeding random noise into the generator and displaying the outputs. Below are a few images generated from random samples. To get a new set of samples, you can re-run the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, subplot_shape):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(*subplot_shape)\n",
    "    for image, ax in zip(images, axes.flatten()):\n",
    "        image = image[np.array([2,1,0]),:,:]\n",
    "        image = np.rollaxis(image / 2 + 0.5, 0, 3)\n",
    "        ax.imshow(image, vmin=-1.0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "noise = noise_sample(36)\n",
    "images = G_output.eval({G_input: noise})\n",
    "plot_images(images, subplot_shape=[6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger number of iterations should generate more realistic looking images. A sampling of such generated images is shown below.\n",
    "\n",
    "[placeholder for W-GAN slow mode results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation (LS-GAN)\n",
    "\n",
    "Since the generator and discriminator architectures of LS-GAN is the same as W-GAN, we will reuse the generator and the discriminator we defined for W-GAN. The main difference between W-GAN and LS-GAN is their loss function and optimizer they use. We redefine the training parameters for LS-GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "minibatch_size = 64\n",
    "num_minibatches = 1000 if isFast else 20000\n",
    "lr = 0.0001\n",
    "momentum = 0.5\n",
    "lambda_ = 0.0002 # lambda in LS-GAN loss function, controls the size of margin\n",
    "weight_decay = 0.00005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph\n",
    "As we mentioned above, one of the differences between LS-GAN and W-GAN is there loss function. In `build_LSGAN_graph`, we should define the loss function for the generator and the discriminator. Another difference is that we do not do weight clipping in LS-GAN, so `clipped_D_parames` is no longer needed. Instead, we use weight decay which is mathematically equivalent to adding an l2 regularization in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_LSGAN_graph(noise_shape, image_shape, generator, discriminator):\n",
    "    \n",
    "    input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "    Z = C.input_variable(noise_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real = C.input_variable(image_shape, dynamic_axes=input_dynamic_axes)\n",
    "    X_real_scaled = (X_real - 127.5) / 127.5\n",
    "\n",
    "    # Create the model function for the generator and discriminator models\n",
    "    X_fake = generator(Z)\n",
    "    D_real = discriminator(X_real_scaled)\n",
    "    D_fake = D_real.clone(\n",
    "        method = 'share',\n",
    "        substitutions = {X_real_scaled.output: X_fake.output}\n",
    "    )\n",
    "    \n",
    "    G_loss = D_fake\n",
    "    D_loss = C.element_max(D_real - D_fake + lambda_ * C.reduce_sum(C.abs(X_fake - X_real_scaled)), [0.])\n",
    "    \n",
    "    G_learner = C.adam(\n",
    "            parameters = X_fake.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            l2_regularization_weight=weight_decay,\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "    )\n",
    "            \n",
    "    D_learner = C.adam(\n",
    "            parameters = D_real.parameters,\n",
    "            lr = C.learning_rate_schedule(lr, C.UnitType.sample),\n",
    "            momentum = C.momentum_schedule(momentum),\n",
    "            variance_momentum = C.momentum_schedule(0.999),\n",
    "            l2_regularization_weight=0.00005,\n",
    "            unit_gain=False,\n",
    "            use_mean_gradient=True\n",
    "        )\n",
    "    \n",
    "    # Instantiate the trainers\n",
    "    G_trainer = C.Trainer(X_fake,\n",
    "                        (G_loss, None),\n",
    "                        G_learner)\n",
    "    D_trainer = C.Trainer(D_real,\n",
    "                        (D_loss, None),\n",
    "                        D_learner)\n",
    "\n",
    "    return X_real, X_fake, Z, G_trainer, D_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "To train the LS-GAN model, we can just simply update the discriminator and the generator alternatively at each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LSGAN(reader_train, generator, discriminator):\n",
    "    X_real, X_fake, Z, G_trainer, D_trainer = \\\n",
    "        build_LSGAN_graph(g_input_dim, d_input_dim, generator, discriminator)\n",
    "        \n",
    "    # print out loss for each model for upto 25 times\n",
    "    print_frequency_mbsize = num_minibatches // 25\n",
    "    \n",
    "    print(\"First row is Generator loss, second row is Discriminator loss\")\n",
    "    pp_G = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    pp_D = C.logging.ProgressPrinter(print_frequency_mbsize)\n",
    "    \n",
    "    \n",
    "    input_map = {X_real: reader_train.streams.features}\n",
    "\n",
    "    for training_step in range(num_minibatches):\n",
    "        # Train the discriminator and the generator alternatively\n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        X_data = reader_train.next_minibatch(minibatch_size, input_map)\n",
    "        batch_inputs = {X_real: X_data[X_real].data, Z: Z_data}\n",
    "        D_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        Z_data = noise_sample(minibatch_size)\n",
    "        batch_inputs = {Z: Z_data}\n",
    "        G_trainer.train_minibatch(batch_inputs)\n",
    "        \n",
    "        pp_G.update_with_trainer(G_trainer)\n",
    "        pp_D.update_with_trainer(D_trainer)\n",
    "\n",
    "    G_trainer_loss = G_trainer.previous_minibatch_loss_average\n",
    "    return Z, X_fake, G_trainer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader_train = create_reader(train_file, True)\n",
    "\n",
    "# G_input, G_output, G_trainer_loss = train(reader_train, dense_generator, dense_discriminator)\n",
    "G_input, G_output, G_trainer_loss = train_LSGAN(reader_train,\n",
    "                                          convolutional_generator,\n",
    "                                          convolutional_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the generator loss \n",
    "print(\"Training loss of the generator is: {0:.2f}\".format(G_trainer_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake (Synthetic) Images (LS-GAN)\n",
    "Now that we have trained the LS-GAN model, we can create fake images simply by feeding random noise into the generator and displaying the outputs. Below are a few images generated from random samples. To get a new set of samples, you can re-run the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, subplot_shape):\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(*subplot_shape)\n",
    "    for image, ax in zip(images, axes.flatten()):\n",
    "        image = image[np.array([2,1,0]),:,:]\n",
    "        image = np.rollaxis(image / 2 + 0.5, 0, 3)\n",
    "        ax.imshow(image, vmin=-1.0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "noise = noise_sample(36)\n",
    "images = G_output.eval({G_input: noise})\n",
    "plot_images(images, subplot_shape=[6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger number of iterations should generate more realistic looking images. A sampling of such generated images is shown below.\n",
    "\n",
    "[placeholder for LS-GAN slow mode results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
