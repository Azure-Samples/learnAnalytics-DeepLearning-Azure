{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Groceries Using Fast R-CNN\n",
    "\n",
    "This notebook demonstrates how to evaluate a single image using Fast R-CNN in CNTK.\n",
    "\n",
    "Please make sure you have the latest Anaconda installed, and create a Python 3.4 environment using the provided `environment.yml` file. Take a look at the `README.md` for more detailed instructions.\n",
    "\n",
    "Training the model is a time-consuming and GPU-intensive process which we've done separately in the cloud using several Python scripts, please see the [CNTK Fast R-CNN example](https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection) or our [original code story](https://www.microsoft.com/developerblog/2017/07/31/using-object-detection-complex-image-classification-scenarios/) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Make sure that the model exists\n",
    "\n",
    "First things first - we will make sure that the Fast-RCNN model file exists. The script will use your local trained model (if available), or will download and use the pre-trained model if a local trained model isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cntk\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Reset this to your own path if you don't want it going into the current directory\n",
    "root_dir = os.path.abspath(os.getcwd())\n",
    "model_path = os.path.join(root_dir, 'Models', 'FastRCNN.model')\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "model_url = 'https://www.cntk.ai/Models/FRCN_Grocery/Fast-RCNN_grocery100.model'\n",
    "if not os.path.exists(model_path):\n",
    "    print('Downloading model from ' + model_url + ', may take a while...')\n",
    "    urlretrieve(model_url, model_path)\n",
    "    print('Saved model as ' + model_path)\n",
    "else:\n",
    "    print('CNTK model already available at ' + model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Load the model and prepare it for evaluation:\n",
    "\n",
    "As a first step for using the Fast-RCNN model, we load the trained model file.\n",
    "\n",
    "The trained model accepts 3 inputs: The image data, the bounding box (region of interest, or ROI) proposals and the ground truth labels of the ROIs. Since we are evaluating a new image - we probably don't have the ground truth labels for the image, hence - we need to adjust the network to accept  only the image and the ROIs as input.\n",
    "\n",
    "In order to do that we use the CNTK APIs to clone the network and change its input nodes.\n",
    "\n",
    "More information and examples regarding cloning nodes of a network are available in the <a href=\"https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_301_Image_Recognition_with_Deep_Transfer_Learning.ipynb\" target=\"_blank\">Transfer Learning</a> tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import load_model\n",
    "from cntk import placeholder\n",
    "from cntk.logging.graph import find_by_name, get_node_outputs\n",
    "from cntk.ops import combine\n",
    "from cntk.ops.sequence import input_variable\n",
    "from cntk.ops.functions import CloneMethod\n",
    "\n",
    "# load the trained model\n",
    "trained_frcnn_model = load_model(model_path)\n",
    "\n",
    "# find the original features and rois input nodes\n",
    "features_node = find_by_name(trained_frcnn_model, \"features\")\n",
    "rois_node = find_by_name(trained_frcnn_model, \"rois\")\n",
    "\n",
    "#  find the final feature layer (named 'z')\n",
    "z_node = find_by_name(trained_frcnn_model, 'z')\n",
    "\n",
    "# define new input nodes for the features (image) and rois\n",
    "image_input = input_variable(features_node.shape, name='features')\n",
    "roi_input = input_variable(rois_node.shape, name='rois')\n",
    "\n",
    "# Clone the desired layers with fixed weights and place holder for the new input nodes\n",
    "cloned_nodes = combine([z_node.owner]).clone(\n",
    "    CloneMethod.freeze,\n",
    "    {features_node: placeholder(name='features'), rois_node: placeholder(name='rois')})\n",
    "\n",
    "# apply the cloned nodes to the input nodes\n",
    "frcnn_model = cloned_nodes(image_input, roi_input)\n",
    "\n",
    "print(\"Fast-RCNN Grocery model loaded and cloned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Load an image and convert it to the expected format\n",
    "\n",
    "Next, we load an image from the test set using OpenCV, and then resize according to the network input dimensions. (Which are set when the network is trained).\n",
    "\n",
    "When resizing, we preserve scale and pad the border areas with a constant value (114), which is later used for normalization by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_height = 1000\n",
    "image_width = 1000 \n",
    "\n",
    "def resize_and_pad(img, width, height, pad_value=114):\n",
    "    img_height, img_width, _ = img.shape\n",
    "\n",
    "    # Scale image by width else height\n",
    "    if img_width > img_height:\n",
    "        target_w = width\n",
    "        target_h = int(np.round(img_height * width / float(img_width)))\n",
    "    else:\n",
    "        target_w = int(np.round(img_width * height / float(img_height)))\n",
    "        target_h = height\n",
    "        \n",
    "    resized = cv2.resize(img, (target_w, target_h), 0, 0, interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    top = int(max(0, np.round((height - target_h) / 2.)))\n",
    "    left = int(max(0, np.round((width - target_w) / 2.)))\n",
    "    \n",
    "    bottom = height - top - target_h\n",
    "    right = width - left - target_w\n",
    "    \n",
    "    resized_with_pad = cv2.copyMakeBorder(resized, top, bottom, left, right, \n",
    "                                          cv2.BORDER_CONSTANT, value=[pad_value, pad_value, pad_value])\n",
    "\n",
    "    #tranpose(2,0,1) converts the image to the HWC format which CNTK accepts\n",
    "    model_arg_rep = np.ascontiguousarray(np.array(resized_with_pad, dtype=np.float32).transpose(2,0,1))\n",
    "    \n",
    "    return resized_with_pad, model_arg_rep\n",
    "\n",
    "def load_image_and_scale(image_path, width, height, pad_value=114):\n",
    "    img = cv2.imread(image_path)\n",
    "    return resize_and_pad(img, width, height, pad_value), img\n",
    "\n",
    "test_image_path = os.path.join(root_dir, 'Grocery/testImages/WIN_20160803_11_28_42_Pro.jpg')\n",
    "(test_img, test_img_model_arg), original_img = load_image_and_scale(test_image_path, image_width, image_height)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Generate ROIs for testing\n",
    "\n",
    "Now, we produce regions of interest (ROIs) proposals using selective search & grid methods, using custom C++ code published as part of the [Fast R-CNN GitHub](https://github.com/rbgirshick/fast-rcnn).\n",
    "\n",
    "Each ROI is in the format of [x,y,w,h], where the coordinates real numbers in the range of 0 to 1, and scaled according to the resized and padded image. \n",
    "\n",
    "The ROIs array is padded with regions of [0,0,0,0] at the end to match the 100 ROIs expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo ln -s /usr/lib/x86_64-linux-gnu/libpython3.5m.so.1.0 /usr/lib/libpython3.4m.so.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cntk_helpers import imArrayWidthHeight, getSelectiveSearchRois, imresizeMaxDim\n",
    "from cntk_helpers import getGridRois, filterRois, roiTransformPadScaleParams, roiTransformPadScale\n",
    "\n",
    "# Constants - See CNTK Fast R-CNN Example for definitions\n",
    "roi_min_dim_rel, roi_max_dim_rel = 0.04, 0.4\n",
    "roi_min_nr_pixels_rel, roi_max_nr_pixels_rel = 2. * roi_min_dim_rel**2., 0.33 * roi_max_dim_rel**2.\n",
    "roi_max_aspect, roi_max_img_dim = 4., 200\n",
    "ss_scale, ss_sigma, ss_min_size = 100, 1.2, 20\n",
    "grid_nr_scales, grid_aspects = 7, [1., 2., 0.5]\n",
    "cntk_nr_rois, cntk_pad_width, cntk_pad_height = 100, 1000, 1000\n",
    "\n",
    "def get_rois_for_image(img):\n",
    "    roi_min_dim, roi_max_dim = roi_min_dim_rel * roi_max_img_dim, roi_max_dim_rel * roi_max_img_dim\n",
    "    roi_min_nr_pixels, roi_max_nr_pixels = roi_min_nr_pixels_rel * roi_max_img_dim**2, roi_max_nr_pixels_rel * roi_max_img_dim**2\n",
    "    img_orig = img.copy()\n",
    "\n",
    "    # get rois\n",
    "    print (\"Calling selective search..\")\n",
    "    rects, scaled_img, scale = getSelectiveSearchRois(img_orig, ss_scale, ss_sigma, ss_min_size, roi_max_img_dim)\n",
    "    print (\"Number of rois detected using selective search: %d\" % len(rects))\n",
    "        \n",
    "    # add grid rois\n",
    "    scaled_width, scaled_height = imArrayWidthHeight(scaled_img)\n",
    "    rects_grid = getGridRois(scaled_width, scaled_height, grid_nr_scales, grid_aspects)\n",
    "    print (\"Number of rois on grid added: %d\" % len(rects_grid))\n",
    "    rects += rects_grid\n",
    "\n",
    "    # run filter\n",
    "    print (\"Number of rectangles before filtering  = %d\" % len(rects))\n",
    "    rois = filterRois(rects, scaled_width, scaled_height, roi_min_nr_pixels, roi_max_nr_pixels, roi_min_dim, roi_max_dim, roi_max_aspect)\n",
    "    if len(rois) == 0: #make sure at least one roi returned per image\n",
    "        rois = [[5, 5, scaled_width-5, scaled_height-5]]\n",
    "    print (\"Number of rectangles after filtering  = %d\" % len(rois))\n",
    "\n",
    "    # scale up to original size\n",
    "    # note: each rectangle is in original image format with [x1,y1,x2,y2]\n",
    "    original_rois = np.int32(np.array(rois) / scale)\n",
    "    \n",
    "    orig_height, orig_width, _ = img.shape\n",
    "    # all rois need to be scaled + padded to cntk input image size\n",
    "    targetw, targeth, w_offset, h_offset, scale = \\\n",
    "        roiTransformPadScaleParams(orig_width, orig_height, cntk_pad_width, cntk_pad_height)\n",
    "    \n",
    "    rois = []\n",
    "    for original_roi in original_rois:\n",
    "        x1, y1, x2, y2 = roiTransformPadScale(original_roi, w_offset, h_offset, scale)        \n",
    "        rois.append([x1, y1, x2, y2])\n",
    "    \n",
    "    # pad or prune rois if needed:\n",
    "    if len(rois) < cntk_nr_rois:\n",
    "        rois += [[0, 0, 0, 0]] * (cntk_nr_rois - len(rois))\n",
    "    elif len(rois) > cntk_nr_rois:\n",
    "        rois = rois[:cntk_nr_rois]\n",
    "    return np.array(rois), original_rois\n",
    "\n",
    "test_rois, original_rois = get_rois_for_image(original_img)\n",
    "roi_padding_index = len(original_rois)\n",
    "print(\"Number of rois for evaluation: %d\" % len(test_rois))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Evaluate the sample\n",
    "\n",
    "Here, we prepare the data to be in CNTK's expected arguments format and run it through the model used the model's `eval` method.\n",
    "\n",
    "We then process the result by trimming the padded ROIs, and calculate the predicted labels and their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk_helpers import softmax2D\n",
    "\n",
    "# a dummy variable for labels the will be given as an input to the network but will be ignored\n",
    "dummy_labels = np.zeros((2000,17))\n",
    "\n",
    "#Index the names of the arguments so we can get them by name\n",
    "args_indices = dict([(arg.name, i) for (i, arg) in enumerate(frcnn_model.arguments)])\n",
    "    \n",
    "# prepare the arguments\n",
    "arguments = {\n",
    "    frcnn_model.arguments[args_indices['features']]: [test_img_model_arg],\n",
    "    frcnn_model.arguments[args_indices['rois']]: [np.float32(test_rois)],\n",
    "}\n",
    "\n",
    "# run it through the model\n",
    "output = frcnn_model.eval(arguments)\n",
    "\n",
    "# we now extract the features from the layer just before the softmax layer\n",
    "rois_values = output[0][0][:roi_padding_index]\n",
    "\n",
    "# get the prediction for each roi by taking the index with the maximal value in each row \n",
    "rois_labels_predictions = np.argmax(rois_values, axis=1)\n",
    "\n",
    "# calculate the probabilities using softmax2D\n",
    "rois_probs = softmax2D(rois_values)\n",
    "\n",
    "# print the number of ROIs that were detected as non-background\n",
    "print(\"Number of detections: %d\" % np.sum(rois_labels_predictions > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Merge overlapping regions using Non-Maxima-Suppression\n",
    "\n",
    "Before inspecting the predictions, we need to merge overlapping regions that were detected using the Non-Maxima-Suppression algorithm that is implemented in the cntk_helpers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk_helpers import applyNonMaximaSuppression\n",
    "nms_threshold = 0.1\n",
    "non_padded_rois = test_rois[:roi_padding_index]\n",
    "max_probs = np.amax(rois_probs, axis=1).tolist()\n",
    "\n",
    "rois_prediction_indices = applyNonMaximaSuppression(nms_threshold, rois_labels_predictions, max_probs, non_padded_rois, ignore_background=True)\n",
    "print(\"Indices of selected regions:\",rois_prediction_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualize the results\n",
    "\n",
    "As a final step, we use the OpenCV **rectangle** and **putText** methods in order to draw the selected regions on the original image alongside their corresponding predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_with_prediction = test_rois[rois_prediction_indices]\n",
    "rois_prediction_labels = rois_labels_predictions[rois_prediction_indices]\n",
    "rois_predicion_scores = rois_values[rois_prediction_indices]\n",
    "original_rois_predictions = original_rois[rois_prediction_indices]\n",
    "\n",
    "classes = ('__background__',  # always index 0\n",
    "           'avocado', 'orange', 'butter', '  booze', 'eggs', 'pickle', 'yogurt', 'ketchup',\n",
    "           'juice', 'onion', 'pepper', 'tomato', 'water', 'milk', 'tabasco', 'mustard')\n",
    "\n",
    "original_img_cpy = original_img.copy()\n",
    "\n",
    "for roi,label in zip(original_rois_predictions, rois_prediction_labels):\n",
    "    (x1,y1,x2,y2) = roi\n",
    "    cv2.rectangle(original_img_cpy, (x1, y1), (x2, y2), (0, 255, 0), 5)\n",
    "    cv2.putText(original_img_cpy,classes[label],(x1,y2 + 30), cv2.FONT_HERSHEY_DUPLEX, 2,(200,0,255),3,cv2.LINE_AA)\n",
    "\n",
    "print(\"Evaluation result:\")\n",
    "plt.figure(figsize=(10, 10))    \n",
    "plt.imshow(cv2.cvtColor(original_img_cpy, cv2.COLOR_BGR2RGB), interpolation='nearest')\n",
    "\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cntk-py35",
   "language": "python",
   "name": "cntk-py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
