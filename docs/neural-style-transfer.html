<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Deep Learning on Azure</title>
  <meta name="description" content="Rendered version of Deep Learning on Azure materials.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Deep Learning on Azure" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Rendered version of Deep Learning on Azure materials." />
  <meta name="github-repo" content="Azure/learnAnalytics-DeepLearning-Azure" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Deep Learning on Azure" />
  
  <meta name="twitter:description" content="Rendered version of Deep Learning on Azure materials." />
  

<meta name="author" content="Ali Zaidi">


<meta name="date" content="2017-10-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="fashion-mnist.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Deep Learning on Azure</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#part-i---fundamentals-and-azure-for-machine-learning"><i class="fa fa-check"></i><b>1.1</b> Part I - Fundamentals and Azure for Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#part-ii---optimization"><i class="fa fa-check"></i><b>1.2</b> Part II - Optimization</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#part-iii---convolutional-neural-networks"><i class="fa fa-check"></i><b>1.3</b> Part III - Convolutional Neural Networks</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#part-iv---recurrent-networks"><i class="fa fa-check"></i><b>1.4</b> Part IV - Recurrent Networks</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#part-v---reinforcement-learning"><i class="fa fa-check"></i><b>1.5</b> Part V - Reinforcement Learning</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#part-vi---generative-models"><i class="fa fa-check"></i><b>1.6</b> Part VI - Generative Models</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#part-vii---operationalization-methods"><i class="fa fa-check"></i><b>1.7</b> Part VII - Operationalization Methods</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#useful-resources"><i class="fa fa-check"></i><b>1.8</b> Useful Resources</a><ul>
<li class="chapter" data-level="1.8.1" data-path="index.html"><a href="index.html#online-courses"><i class="fa fa-check"></i><b>1.8.1</b> Online Courses</a></li>
<li class="chapter" data-level="1.8.2" data-path="index.html"><a href="index.html#online-books-and-blogs"><i class="fa fa-check"></i><b>1.8.2</b> Online Books and Blogs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html"><i class="fa fa-check"></i><b>2</b> Provisioning Linux DSVMs with Azure CLI 2.0</a><ul>
<li class="chapter" data-level="2.1" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#installing-and-testing-the-azure-cli"><i class="fa fa-check"></i><b>2.1</b> Installing and Testing the Azure CLI</a></li>
<li class="chapter" data-level="2.2" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#login-to-your-azure-account"><i class="fa fa-check"></i><b>2.2</b> Login to Your Azure Account</a></li>
<li class="chapter" data-level="2.3" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#deploying-via-custom-script"><i class="fa fa-check"></i><b>2.3</b> <a name="deploying"></a> Deploying Via Custom Script</a></li>
<li class="chapter" data-level="2.4" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#updating-dsvm-os-disk-size"><i class="fa fa-check"></i><b>2.4</b> Updating DSVM OS Disk Size</a></li>
<li class="chapter" data-level="2.5" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#deploying-manually-only-proceed-if-you-didnt-use-the-script-above"><i class="fa fa-check"></i><b>2.5</b> Deploying Manually <strong>(Only Proceed if You Didn’t Use the Script Above!)</strong></a><ul>
<li class="chapter" data-level="2.5.1" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#create-a-new-resource-group"><i class="fa fa-check"></i><b>2.5.1</b> Create a New Resource Group</a></li>
<li class="chapter" data-level="2.5.2" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#create-your-dsvm"><i class="fa fa-check"></i><b>2.5.2</b> Create Your DSVM</a></li>
<li class="chapter" data-level="2.5.3" data-path="provisioning-linux-dsvms-with-azure-cli-2-0.html"><a href="provisioning-linux-dsvms-with-azure-cli-2-0.html#create-a-password-for-the-user"><i class="fa fa-check"></i><b>2.5.3</b> Create a Password for the User</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html"><i class="fa fa-check"></i><b>3</b> Upgrading CNTK and CUDNN</a><ul>
<li class="chapter" data-level="3.1" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#updating-cntk"><i class="fa fa-check"></i><b>3.1</b> Updating CNTK</a></li>
<li class="chapter" data-level="3.2" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#updating-cntk-with-a-single-script"><i class="fa fa-check"></i><b>3.2</b> Updating CNTK With a Single Script</a><ul>
<li class="chapter" data-level="3.2.1" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#launch-jupyterlab"><i class="fa fa-check"></i><b>3.2.1</b> Launch JupyterLab</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#manually-updating-cntk-and-launching-jupyter-no-need-to-do-this-if-you-use-the-script"><i class="fa fa-check"></i><b>3.3</b> Manually Updating CNTK and Launching Jupyter (<strong>No Need to Do This if You Use the Script</strong>)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#create-conda-virtual-environment"><i class="fa fa-check"></i><b>3.3.1</b> Create Conda Virtual Environment</a></li>
<li class="chapter" data-level="3.3.2" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#install-cntk-using-pip-binary-wheels"><i class="fa fa-check"></i><b>3.3.2</b> Install CNTK Using <code>pip</code> Binary Wheels</a></li>
<li class="chapter" data-level="3.3.3" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#temporary-fixes"><i class="fa fa-check"></i><b>3.3.3</b> Temporary Fixes</a></li>
<li class="chapter" data-level="3.3.4" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#conda-extensions"><i class="fa fa-check"></i><b>3.3.4</b> Conda Extensions</a></li>
<li class="chapter" data-level="3.3.5" data-path="upgrading-cntk-and-cudnn.html"><a href="upgrading-cntk-and-cudnn.html#install-keras"><i class="fa fa-check"></i><b>3.3.5</b> Install Keras</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html"><i class="fa fa-check"></i><b>4</b> Transfer Learning with CNTK</a><ul>
<li class="chapter" data-level="4.1" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#classifying-dogs-and-cats-using-a-pre-trained-network"><i class="fa fa-check"></i><b>4.1</b> Classifying Dogs and Cats Using a Pre-Trained Network</a><ul>
<li class="chapter" data-level="4.1.1" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#download-data"><i class="fa fa-check"></i><b>4.1.1</b> Download Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#create-train-test-and-validate-sets"><i class="fa fa-check"></i><b>4.2</b> Create Train, Test and Validate Sets</a></li>
<li class="chapter" data-level="4.3" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#kittypupsploration"><i class="fa fa-check"></i><b>4.3</b> Kittypupsploration</a></li>
<li class="chapter" data-level="4.4" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#data-readers-in-cntk"><i class="fa fa-check"></i><b>4.4</b> Data Readers in CNTK</a></li>
<li class="chapter" data-level="4.5" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#pre-trained-models"><i class="fa fa-check"></i><b>4.5</b> Pre-Trained Models</a></li>
<li class="chapter" data-level="4.6" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#defining-model-architecture"><i class="fa fa-check"></i><b>4.6</b> Defining Model Architecture</a></li>
<li class="chapter" data-level="4.7" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#training-the-transfer-model"><i class="fa fa-check"></i><b>4.7</b> Training the Transfer Model</a></li>
<li class="chapter" data-level="4.8" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#define-minibatch-source"><i class="fa fa-check"></i><b>4.8</b> Define Minibatch Source</a></li>
<li class="chapter" data-level="4.9" data-path="transfer-learning-with-cntk.html"><a href="transfer-learning-with-cntk.html#training"><i class="fa fa-check"></i><b>4.9</b> Training</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fashion-mnist.html"><a href="fashion-mnist.html"><i class="fa fa-check"></i><b>5</b> Fashion MNIST</a><ul>
<li class="chapter" data-level="5.1" data-path="fashion-mnist.html"><a href="fashion-mnist.html#import-core-libraries-and-specify-keras-backend"><i class="fa fa-check"></i><b>5.1</b> Import Core Libraries and Specify Keras Backend</a></li>
<li class="chapter" data-level="5.2" data-path="fashion-mnist.html"><a href="fashion-mnist.html#downloading-fashion-dataset"><i class="fa fa-check"></i><b>5.2</b> Downloading Fashion Dataset</a></li>
<li class="chapter" data-level="5.3" data-path="fashion-mnist.html"><a href="fashion-mnist.html#scale-data-and-visualize"><i class="fa fa-check"></i><b>5.3</b> Scale Data and Visualize</a></li>
<li class="chapter" data-level="5.4" data-path="fashion-mnist.html"><a href="fashion-mnist.html#create-network-architecture-and-model-parameters"><i class="fa fa-check"></i><b>5.4</b> Create Network Architecture and Model Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="fashion-mnist.html"><a href="fashion-mnist.html#training-our-model"><i class="fa fa-check"></i><b>5.5</b> Training Our Model</a></li>
<li class="chapter" data-level="5.6" data-path="fashion-mnist.html"><a href="fashion-mnist.html#scoring-the-model"><i class="fa fa-check"></i><b>5.6</b> Scoring the Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-style-transfer.html"><a href="neural-style-transfer.html"><i class="fa fa-check"></i><b>6</b> Neural Style Transfer</a><ul>
<li class="chapter" data-level="6.0.1" data-path="neural-style-transfer.html"><a href="neural-style-transfer.html#defining-the-loss-function"><i class="fa fa-check"></i><b>6.0.1</b> Defining the loss function</a></li>
<li class="chapter" data-level="6.0.2" data-path="neural-style-transfer.html"><a href="neural-style-transfer.html#instantiating-the-loss"><i class="fa fa-check"></i><b>6.0.2</b> Instantiating the loss</a></li>
<li class="chapter" data-level="6.0.3" data-path="neural-style-transfer.html"><a href="neural-style-transfer.html#optimizing-the-loss"><i class="fa fa-check"></i><b>6.0.3</b> Optimizing the loss</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://aka.ms/az-dl" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning on Azure</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-style-transfer" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Neural Style Transfer</h1>
<p>This tutorial shows how to transfer the style / texture of one image to another. This allows us to take our ordinary photos and render them in the style of famous images or paintings.</p>
<p>Apart from creating nice looking pictures, in this tutorial you will learn how to load a pretrained <a href="https://arxiv.org/abs/1409.1556">VGG model</a> into CNTK, how to get the gradient of a function with respect to an input variable (rather than a parameter), and how to use the gradient outside of CNTK.</p>
<p>We will follow the approach of <a href="https://arxiv.org/abs/1508.06576">Gatys et. al.</a> with some of the improvements in <a href="https://arxiv.org/abs/1605.04603">Novak and Nikulin</a>. While <a href="https://arxiv.org/abs/1603.08155">faster techniques</a> exist, these are limited to transfering a particular style.</p>
<p>We begin by importing the necessary packages. In addition to the usual suspects (<code>numpy</code>, <code>scipy</code>, and <code>cntk</code>) we will need <code>PIL</code> to work with images, <code>requests</code> to download a pretrained model and <code>h5py</code> to read in the weights of the pretrained model.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> __future__ <span class="im">import</span> print_function
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">from</span> scipy <span class="im">import</span> optimize <span class="im">as</span> opt
<span class="im">import</span> cntk <span class="im">as</span> C
<span class="im">from</span> PIL <span class="im">import</span> Image
<span class="im">import</span> requests
<span class="im">import</span> h5py
<span class="im">import</span> os
<span class="op">%</span>matplotlib inline
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="co"># Select the right target device when this notebook is being tested:</span>
<span class="cf">if</span> <span class="st">&#39;TEST_DEVICE&#39;</span> <span class="kw">in</span> os.environ:
    <span class="cf">if</span> os.environ[<span class="st">&#39;TEST_DEVICE&#39;</span>] <span class="op">==</span> <span class="st">&#39;cpu&#39;</span>:
        C.device.try_set_default_device(C.device.cpu())
    <span class="cf">else</span>:
        C.device.try_set_default_device(C.device.gpu(<span class="dv">0</span>))</code></pre></div>
<p>The pretrained model is a VGG network which we originally got from <a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3">this page</a>. We host it in a place which permits easy downloading. Below we download it if it is not already available locally and load the weights into numpy arrays.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> download(url, filename):
    response <span class="op">=</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>)
    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> handle:
        <span class="cf">for</span> data <span class="kw">in</span> response.iter_content(chunk_size<span class="op">=</span><span class="dv">2</span><span class="op">**</span><span class="dv">20</span>):
            <span class="cf">if</span> data: handle.write(data)


<span class="kw">def</span> load_vgg(path):
    f <span class="op">=</span> h5py.File(path)
    layers <span class="op">=</span> []
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(f.attrs[<span class="st">&#39;nb_layers&#39;</span>]):
        g <span class="op">=</span> f[<span class="st">&#39;layer_</span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(k)]
        n <span class="op">=</span> g.attrs[<span class="st">&#39;nb_params&#39;</span>]
        layers.append([g[<span class="st">&#39;param_</span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(p)][:] <span class="cf">for</span> p <span class="kw">in</span> <span class="bu">range</span>(n)])
    f.close()
    <span class="cf">return</span> layers

<span class="co"># Check for an environment variable defined in CNTK&#39;s test infrastructure</span>
envvar <span class="op">=</span> <span class="st">&#39;CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY&#39;</span>
<span class="kw">def</span> is_test(): <span class="cf">return</span> envvar <span class="kw">in</span> os.environ

path <span class="op">=</span> <span class="st">&#39;vgg16_weights.bin&#39;</span>
url <span class="op">=</span> <span class="st">&#39;https://cntk.ai/jup/models/vgg16_weights.bin&#39;</span>
<span class="co"># We check for the model locally</span>
<span class="cf">if</span> <span class="kw">not</span> os.path.exists(path):
    <span class="co"># If not there we might be running in CNTK&#39;s test infrastructure</span>
    <span class="cf">if</span> is_test():
        path <span class="op">=</span> os.path.join(os.environ[envvar],<span class="st">&#39;PreTrainedModels&#39;</span>,<span class="st">&#39;Vgg16&#39;</span>,<span class="st">&#39;v0&#39;</span>,path)
    <span class="cf">else</span>:
        <span class="co">#If neither is true we download the file from the web</span>
        <span class="bu">print</span>(<span class="st">&#39;downloading VGG model (~0.5GB)&#39;</span>)
        download(url, path)
layers <span class="op">=</span> load_vgg(path)
<span class="bu">print</span>(<span class="st">&#39;loaded VGG model&#39;</span>)</code></pre></div>
<pre><code>downloading VGG model (~0.5GB)
loaded VGG model</code></pre>
<p>Next we define the VGG network as a CNTK graph.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># A convolutional layer in the VGG network</span>
<span class="kw">def</span> vggblock(x, arrays, layer_map, name):
    f <span class="op">=</span> arrays[<span class="dv">0</span>]
    b <span class="op">=</span> arrays[<span class="dv">1</span>]
    k <span class="op">=</span> C.constant(value<span class="op">=</span>f)
    t <span class="op">=</span> C.constant(value<span class="op">=</span>np.reshape(b, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)))
    y <span class="op">=</span> C.relu(C.convolution(k, x, auto_padding<span class="op">=</span>[<span class="va">False</span>, <span class="va">True</span>, <span class="va">True</span>]) <span class="op">+</span> t)
    layer_map[name] <span class="op">=</span> y
    <span class="cf">return</span> y

<span class="co"># A pooling layer in the VGG network</span>
<span class="kw">def</span> vggpool(x):
    <span class="cf">return</span> C.pooling(x, C.AVG_POOLING, (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">2</span>, <span class="dv">2</span>))


<span class="co"># Build the graph for the VGG network (excluding fully connected layers)</span>
<span class="kw">def</span> model(x, layers):
    model_layers <span class="op">=</span> {}
    <span class="kw">def</span> convolutional(z): <span class="cf">return</span> <span class="bu">len</span>(z) <span class="op">==</span> <span class="dv">2</span> <span class="kw">and</span> <span class="bu">len</span>(z[<span class="dv">0</span>].shape) <span class="op">==</span> <span class="dv">4</span>
    conv <span class="op">=</span> [layer <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">if</span> convolutional(layer)]
    cnt <span class="op">=</span> <span class="dv">0</span>
    num_convs <span class="op">=</span> {<span class="dv">1</span>: <span class="dv">2</span>, <span class="dv">2</span>: <span class="dv">2</span>, <span class="dv">3</span>: <span class="dv">3</span>, <span class="dv">4</span>: <span class="dv">3</span>, <span class="dv">5</span>: <span class="dv">3</span>}
    <span class="cf">for</span> outer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">6</span>):
        <span class="cf">for</span> inner <span class="kw">in</span> <span class="bu">range</span>(num_convs[outer]):
            x <span class="op">=</span> vggblock(x, conv[cnt], model_layers, <span class="st">&#39;conv</span><span class="sc">%d</span><span class="st">_</span><span class="sc">%d</span><span class="st">&#39;</span> <span class="op">%</span> (outer, <span class="dv">1</span><span class="op">+</span>inner))
            cnt <span class="op">+=</span> <span class="dv">1</span>
        x <span class="op">=</span> vggpool(x)
    
    <span class="cf">return</span> x, C.combine([model_layers[k] <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">sorted</span>(model_layers.keys())])</code></pre></div>
<div id="defining-the-loss-function" class="section level3">
<h3><span class="header-section-number">6.0.1</span> Defining the loss function</h3>
<p>The interesting part in this line of work is the definition of a loss function that, when optimized, leads to a result that is close to both the content of one image, as well as the style of the other image. This loss contains multiple terms some of which are defined in terms of the VGG network we just created. Concretely, the loss takes a candidate image <span class="math inline">\(x\)</span> and takes a weighted sum of three terms: the content loss, the style loss and the total variation loss: <span class="math display">\[
L(x) = \alpha C(x) + \beta S(x) + T(x)
\]</span> where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are weights on the content loss and the style loss, respectively. We have normalized the weights so that the weight in front of the total variation loss is 1. How are each of these terms computed?</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Total_variation_denoising">total variation loss</a> <span class="math inline">\(T(x)\)</span> is the simplest one to understand: It measures the average sum of squared differences among adjacent pixel values and encourages the result <span class="math inline">\(x\)</span> to be a smooth image. We implement this by convolving the image with a kernel containing (-1,1) both horizontally and vertically, squaring the results and computing their average.</li>
<li>The content loss is measuring the squared difference between the content image and <span class="math inline">\(x\)</span>. We can measure this difference on the raw pixels or at various layers inside the VGG network. While we write the content loss as <span class="math inline">\(C(x)\)</span> it implicitly depends on the content image we provide. However since that image is fixed we do not write this dependence explicitly.</li>
<li>The style loss <span class="math inline">\(S(x)\)</span> is similar to the content loss in that it also implicitly depends on another image. The main idea of Gatys et. al. was to define the style as the correlations among the activations of the network and measure the style loss as the squared difference between these correlations. In particular for a particular layer we compute a covariance matrix among the output channels averaging across all positions. The style loss is just the squared error between the covariance matrix induced by the style image and the covariance matrix induced by <span class="math inline">\(x\)</span>. We are deliberately vague here as to which layer of the network is used for creating the covariance loss. Different implementations do this differently and below we will use a weighted sum of all the style losses of all layers.</li>
</ul>
<p>Below we define these loss functions:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> flatten(x):
    <span class="cf">assert</span> <span class="bu">len</span>(x.shape) <span class="op">&gt;=</span> <span class="dv">3</span>
    <span class="cf">return</span> C.reshape(x, (x.shape[<span class="op">-</span><span class="dv">3</span>], x.shape[<span class="op">-</span><span class="dv">2</span>] <span class="op">*</span> x.shape[<span class="op">-</span><span class="dv">1</span>]))


<span class="kw">def</span> gram(x):
    features <span class="op">=</span> C.minus(flatten(x), C.reduce_mean(x))
    <span class="cf">return</span> C.times_transpose(features, features)


<span class="kw">def</span> npgram(x):
    features <span class="op">=</span> np.reshape(x, (<span class="op">-</span><span class="dv">1</span>, x.shape[<span class="op">-</span><span class="dv">2</span>]<span class="op">*</span>x.shape[<span class="op">-</span><span class="dv">1</span>])) <span class="op">-</span> np.mean(x)
    <span class="cf">return</span> features.dot(features.T)


<span class="kw">def</span> style_loss(a, b):
    channels, x, y <span class="op">=</span> a.shape
    <span class="cf">assert</span> x <span class="op">==</span> y
    A <span class="op">=</span> gram(a)
    B <span class="op">=</span> npgram(b)
    <span class="cf">return</span> C.squared_error(A, B)<span class="op">/</span>(channels<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">4</span>)


<span class="kw">def</span> content_loss(a,b):
    channels, x, y <span class="op">=</span> a.shape
    <span class="cf">return</span> C.squared_error(a, b)<span class="op">/</span>(channels<span class="op">*</span>x<span class="op">*</span>y)


<span class="kw">def</span> total_variation_loss(x):
    xx <span class="op">=</span> C.reshape(x, (<span class="dv">1</span>,)<span class="op">+</span>x.shape)
    delta <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], dtype<span class="op">=</span>np.float32)
    kh <span class="op">=</span> C.constant(value<span class="op">=</span>delta.reshape(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>))
    kv <span class="op">=</span> C.constant(value<span class="op">=</span>delta.reshape(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>))
    dh <span class="op">=</span> C.convolution(kh, xx, auto_padding<span class="op">=</span>[<span class="va">False</span>])
    dv <span class="op">=</span> C.convolution(kv, xx, auto_padding<span class="op">=</span>[<span class="va">False</span>])
    avg <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (C.reduce_mean(C.square(dv)) <span class="op">+</span> C.reduce_mean(C.square(dh)))
    <span class="cf">return</span> avg</code></pre></div>
</div>
<div id="instantiating-the-loss" class="section level3">
<h3><span class="header-section-number">6.0.2</span> Instantiating the loss</h3>
<p>Now we are ready to instantiate a loss with two particular images. We will use an image of Portland’s landscape and <a href="https://en.wikipedia.org/wiki/The_Starry_Night">The Starry Night</a> by Vincent van Gogh. We first define a few tuning parameters whose explanation is below: - Depending on whether the code runs on a GPU or a CPU we resize the images to 300 x 300 or 64 x 64 respectively and adjust the number of iterations of optimization to speed up the process and for ease of experimentation. You can use a larger size if you like the results. If you only have a CPU you will have to wait a while. - The content weight and style weight are the main parameters that affect the quality of the resulting image. - The decay factor is a number in (0,1) which decides how to weigh the contribution of each layer. Following <a href="https://arxiv.org/abs/1605.04603">Novak and Nikulin</a>, all layers contribute to both the content loss and the style loss. The content loss weighs the input more heavily and each later layer in the VGG network contributes with a weight that is exponentially smaller with its distance from the input. The style loss weighs the output of the VGG network more heavily and each earlier layer in the VGG network contributes with a weight that is exponentially smaller with its distance from the output. As in Novak and Nikulin we use a decay factor of 0.5. - The inner and outer parameters define how we are going to obtain our final result. We will take <code>outer</code> snapshots during our search for the image that minimizes the loss. Each snapshot will be taken after <code>inner</code> steps of optimization. - Finally, a very important thing to know about our pretrained network is how it was trained. In particular, a constant vector was subtracted from all input images that contained the average value for the red, green, and blue channels in the training set. This makes the inputs zero centered which helps the training procedure. If we do not subtract this vector our images will not look like the training images and this will lead to bad results. This vector is referred to as SHIFT below.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">style_path <span class="op">=</span> <span class="st">&#39;style.jpg&#39;</span>
content_path <span class="op">=</span> <span class="st">&#39;azthinksR.jpg&#39;</span>

start_from_random <span class="op">=</span> <span class="va">False</span>
content_weight <span class="op">=</span> <span class="fl">5.0</span>
style_weight <span class="op">=</span> <span class="fl">1.0</span>
decay <span class="op">=</span> <span class="fl">0.5</span>

<span class="cf">if</span> is_test():
    outer <span class="op">=</span> <span class="dv">2</span>
    inner <span class="op">=</span> <span class="dv">2</span>
    SIZE <span class="op">=</span> <span class="dv">64</span>
<span class="cf">else</span>:
    outer <span class="op">=</span> <span class="dv">10</span>
    inner <span class="op">=</span> <span class="dv">20</span>
    SIZE <span class="op">=</span> <span class="dv">300</span>

SHIFT <span class="op">=</span> np.reshape([<span class="fl">103.939</span>, <span class="fl">116.779</span>, <span class="fl">123.68</span>], (<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)).astype(<span class="st">&#39;f&#39;</span>)

<span class="kw">def</span> load_image(path):
    <span class="cf">with</span> Image.<span class="bu">open</span>(path) <span class="im">as</span> pic:
        hw <span class="op">=</span> pic.size[<span class="dv">0</span>] <span class="op">/</span> <span class="dv">2</span>
        hh <span class="op">=</span> pic.size[<span class="dv">1</span>] <span class="op">/</span> <span class="dv">2</span>
        mh <span class="op">=</span> <span class="bu">min</span>(hw,hh)
        cropped <span class="op">=</span> pic.crop((hw <span class="op">-</span> mh, hh <span class="op">-</span> mh, hw <span class="op">+</span> mh, hh <span class="op">+</span> mh))
        array <span class="op">=</span> np.array(cropped.resize((SIZE,SIZE), Image.BICUBIC), dtype<span class="op">=</span>np.float32)
        <span class="cf">return</span> np.ascontiguousarray(np.transpose(array, (<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)))<span class="op">-</span>SHIFT

<span class="kw">def</span> save_image(img, path):
    sanitized_img <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.minimum(<span class="dv">255</span>, img<span class="op">+</span>SHIFT))
    pic <span class="op">=</span> Image.fromarray(np.uint8(np.transpose(sanitized_img, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))))
    pic.save(path)

<span class="kw">def</span> ordered_outputs(f, binding):
    _, output_dict <span class="op">=</span> f.forward(binding, f.outputs)
    <span class="cf">return</span> [np.squeeze(output_dict[out]) <span class="cf">for</span> out <span class="kw">in</span> f.outputs]

<span class="co"># download the images if they are not available locally</span>
<span class="cf">for</span> local_path <span class="kw">in</span> content_path, style_path:
    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(local_path):
        download(<span class="st">&#39;https://cntk.ai/jup/</span><span class="sc">%s</span><span class="st">&#39;</span> <span class="op">%</span> local_path, local_path)

<span class="co"># Load the images</span>
style   <span class="op">=</span> load_image(style_path)
content <span class="op">=</span> load_image(content_path)

<span class="co"># Display the images</span>
<span class="cf">for</span> img <span class="kw">in</span> content, style:
    plt.figure()
    plt.imshow(np.asarray(np.transpose(img<span class="op">+</span>SHIFT, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)), dtype<span class="op">=</span>np.uint8))

<span class="co"># Push the images through the VGG network </span>
<span class="co"># First define the input and the output</span>
y <span class="op">=</span> C.input_variable((<span class="dv">3</span>, SIZE, SIZE), needs_gradient<span class="op">=</span><span class="va">True</span>)
z, intermediate_layers <span class="op">=</span> model(y, layers)
<span class="co"># Now get the activations for the two images</span>
content_activations <span class="op">=</span> ordered_outputs(intermediate_layers, {y: [[content]]})
style_activations <span class="op">=</span> ordered_outputs(intermediate_layers, {y: [[style]]})
style_output <span class="op">=</span> np.squeeze(z.<span class="bu">eval</span>({y: [[style]]}))

<span class="co"># Finally define the loss</span>
n <span class="op">=</span> <span class="bu">len</span>(content_activations)
total <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>decay<span class="op">**</span>(n<span class="op">+</span><span class="dv">1</span>))<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>decay) <span class="co"># makes sure that changing the decay does not affect the magnitude of content/style</span>
loss <span class="op">=</span> (<span class="fl">1.0</span><span class="op">/</span>total <span class="op">*</span> content_weight <span class="op">*</span> content_loss(y, content) 
         <span class="op">+</span> <span class="fl">1.0</span><span class="op">/</span>total <span class="op">*</span> style_weight <span class="op">*</span> style_loss(z, style_output) 
         <span class="op">+</span> total_variation_loss(y))

<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):
    loss <span class="op">=</span> (loss 
        <span class="op">+</span> decay<span class="op">**</span>(i<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span>total <span class="op">*</span> content_weight <span class="op">*</span> content_loss(intermediate_layers.outputs[i], content_activations[i])
        <span class="op">+</span> decay<span class="op">**</span>(n<span class="op">-</span>i)<span class="op">/</span>total <span class="op">*</span> style_weight   <span class="op">*</span>   style_loss(intermediate_layers.outputs[i], style_activations[i]))</code></pre></div>
<div class="figure">
<img src="output_9_0.png" alt="png" />
<p class="caption">png</p>
</div>
<div class="figure">
<img src="output_9_1.png" alt="png" />
<p class="caption">png</p>
</div>
</div>
<div id="optimizing-the-loss" class="section level3">
<h3><span class="header-section-number">6.0.3</span> Optimizing the loss</h3>
<p>Now we are finally ready to find the image that minimizes the loss we defined. We will use the optimization package in scipy and in particular the LBFGS method. LBFGS is a great optimization procedure which is very popular when computing the full gradient is feasible as is the case here.</p>
<p>Notice that we are computing the gradient with respect to the input. This is quite different from most other use cases where we compute the gradient with respect to the network parameters. By default, input variables do not ask for gradients, however we defined our input variable as</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">y <span class="op">=</span> C.input_variable((<span class="dv">3</span>, SIZE, SIZE), needs_gradient<span class="op">=</span><span class="va">True</span>)</code></pre></div>
<p>which means that CNTK will compute the gradient with respect to this input variable as well.</p>
<p>The rest of the code is straightforward and most of the complexity comes from interacting with the scipy optimization package: - The optimizer works only with vectors of double precision so img2vec takes a (3,SIZE,SIZE) image and converts it to a vector of doubles - CNTK needs the input as an image but scipy is calling us back with a vector - CNTK computes a gradient as an image but scipy wants the gradient as a vector</p>
<p>Besides these complexities we just start from the content image (or a random image), perform our optimization and display the final result.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># utility to convert a vector to an image</span>
<span class="kw">def</span> vec2img(x):
    d <span class="op">=</span> np.<span class="bu">round</span>(np.sqrt(x.size <span class="op">/</span> <span class="dv">3</span>)).astype(<span class="st">&#39;i&#39;</span>)
    <span class="cf">return</span> np.reshape(x.astype(np.float32), (<span class="dv">3</span>, d, d))

<span class="co"># utility to convert an image to a vector</span>
<span class="kw">def</span> img2vec(img):
    <span class="cf">return</span> img.flatten().astype(np.float64)

<span class="co"># utility to compute the value and the gradient of f at a particular place defined by binding</span>
<span class="kw">def</span> value_and_grads(f, binding):
    <span class="cf">if</span> <span class="bu">len</span>(f.outputs) <span class="op">!=</span> <span class="dv">1</span>:
        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&#39;function must return a single tensor&#39;</span>)
    df, valdict <span class="op">=</span> f.forward(binding, [f.output], <span class="bu">set</span>([f.output]))
    value <span class="op">=</span> <span class="bu">list</span>(valdict.values())[<span class="dv">0</span>]
    grads <span class="op">=</span> f.backward(df, {f.output: np.ones_like(value)}, <span class="bu">set</span>(binding.keys()))
    <span class="cf">return</span> value, grads

<span class="co"># an objective function that scipy will be happy with</span>
<span class="kw">def</span> objfun(x, loss):
    y <span class="op">=</span> vec2img(x)
    v, g <span class="op">=</span> value_and_grads(loss, {loss.arguments[<span class="dv">0</span>]: [[y]]})
    v <span class="op">=</span> np.reshape(v, (<span class="dv">1</span>,))
    g <span class="op">=</span> img2vec(<span class="bu">list</span>(g.values())[<span class="dv">0</span>])
    <span class="cf">return</span> v, g

<span class="co"># the actual optimization procedure</span>
<span class="kw">def</span> optimize(loss, x0, inner, outer):
    bounds <span class="op">=</span> [(<span class="op">-</span>np.<span class="bu">min</span>(SHIFT), <span class="dv">255</span><span class="op">-</span>np.<span class="bu">max</span>(SHIFT))]<span class="op">*</span>x0.size
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(outer):
        s <span class="op">=</span> opt.minimize(objfun, img2vec(x0), args<span class="op">=</span>(loss,), method<span class="op">=</span><span class="st">&#39;L-BFGS-B&#39;</span>, 
                         bounds<span class="op">=</span>bounds, options<span class="op">=</span>{<span class="st">&#39;maxiter&#39;</span>: inner}, jac<span class="op">=</span><span class="va">True</span>)
        <span class="bu">print</span>(<span class="st">&#39;objective : </span><span class="sc">%s</span><span class="st">&#39;</span> <span class="op">%</span> s.fun[<span class="dv">0</span>])
        x0 <span class="op">=</span> vec2img(s.x)
        path <span class="op">=</span> <span class="st">&#39;output_</span><span class="sc">%d</span><span class="st">.jpg&#39;</span> <span class="op">%</span> i
        save_image(x0, path)
    <span class="cf">return</span> x0

np.random.seed(<span class="dv">98052</span>)
<span class="cf">if</span> start_from_random:
    x0 <span class="op">=</span> np.random.randn(<span class="dv">3</span>, SIZE, SIZE).astype(np.float32)
<span class="cf">else</span>:
    x0 <span class="op">=</span> content
xstar <span class="op">=</span> optimize(loss, x0, inner, outer)
plt.imshow(np.asarray(np.transpose(xstar<span class="op">+</span>SHIFT, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)), dtype<span class="op">=</span>np.uint8))</code></pre></div>
<pre><code>objective : 70194.4
objective : 60445.3
objective : 57855.2
objective : 56728.0
objective : 56166.7
objective : 55840.4
objective : 55627.4
objective : 55493.8
objective : 55395.1
objective : 55325.0





&lt;matplotlib.image.AxesImage at 0x7fe962aa0e10&gt;</code></pre>
<div class="figure">
<img src="output_11_2.png" alt="png" />
<p class="caption">png</p>
</div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># For testing purposes</span>
objfun(xstar, loss)[<span class="dv">0</span>][<span class="dv">0</span>]</code></pre></div>
<pre><code>55325.039</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fashion-mnist.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Azure/learnAnalytics-DeepLearning-Azure/edit/master/05-Neural_Style.Rmd",
"text": "Edit"
},
"download": ["azure-deep-learning.pdf", "azure-deep-learning.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
