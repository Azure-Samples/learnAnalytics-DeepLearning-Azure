{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Character Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.learners import momentum_sgd, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk.ops import sequence\n",
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.metrics import classification_error\n",
    "from cntk.ops.functions import load_model\n",
    "from cntk.layers import LSTM, Stabilizer, Recurrence, Dense, For, Sequential\n",
    "from cntk.logging import log_number_of_parameters, ProgressPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "minibatch_size = 100 # also how much time we unroll the RNN for\n",
    "\n",
    "# Get data\n",
    "def get_data(p, minibatch_size, data, char_to_ix, vocab_dim):\n",
    "\n",
    "    # the character LM predicts the next character so get sequences offset by 1\n",
    "    xi = [char_to_ix[ch] for ch in data[p:p+minibatch_size]]\n",
    "    yi = [char_to_ix[ch] for ch in data[p+1:p+minibatch_size+1]]\n",
    "    \n",
    "    # a slightly inefficient way to get one-hot vectors but fine for low vocab (like char-lm)\n",
    "    X = np.eye(vocab_dim, dtype=np.float32)[xi]\n",
    "    Y = np.eye(vocab_dim, dtype=np.float32)[yi]\n",
    "\n",
    "    # return a list of numpy arrays for each of X (features) and Y (labels)\n",
    "    return [X], [Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the network\n",
    "def sample(root, ix_to_char, vocab_dim, char_to_ix, prime_text='', use_hardmax=True, length=100, temperature=1.0):\n",
    "\n",
    "    # temperature: T < 1 means smoother; T=1.0 means same; T > 1 means more peaked\n",
    "    def apply_temp(p):\n",
    "        # apply temperature\n",
    "        p = np.power(p, (temperature))\n",
    "        # renormalize and return\n",
    "        return (p / np.sum(p))\n",
    "\n",
    "    def sample_word(p):\n",
    "        if use_hardmax:\n",
    "            w = np.argmax(p, axis=2)[0,0]\n",
    "        else:\n",
    "            # normalize probabilities then take weighted sample\n",
    "            p = np.exp(p) / np.sum(np.exp(p))            \n",
    "            p = apply_temp(p)\n",
    "            w = np.random.choice(range(vocab_dim), p=p.ravel())\n",
    "        return w\n",
    "\n",
    "    plen = 1\n",
    "    prime = -1\n",
    "\n",
    "    # start sequence with first input    \n",
    "    x = np.zeros((1, vocab_dim), dtype=np.float32)    \n",
    "    if prime_text != '':\n",
    "        plen = len(prime_text)\n",
    "        prime = char_to_ix[prime_text[0]]\n",
    "    else:\n",
    "        prime = np.random.choice(range(vocab_dim))\n",
    "    x[0, prime] = 1\n",
    "    arguments = ([x], [True])\n",
    "\n",
    "    # setup a list for the output characters and add the initial prime text\n",
    "    output = []\n",
    "    output.append(prime)\n",
    "    \n",
    "    # loop through prime text\n",
    "    for i in range(plen):            \n",
    "        p = root.eval(arguments)        \n",
    "        \n",
    "        # reset\n",
    "        x = np.zeros((1, vocab_dim), dtype=np.float32)\n",
    "        if i < plen-1:\n",
    "            idx = char_to_ix[prime_text[i+1]]\n",
    "        else:\n",
    "            idx = sample_word(p)\n",
    "\n",
    "        output.append(idx)\n",
    "        x[0, idx] = 1            \n",
    "        arguments = ([x], [False])\n",
    "    \n",
    "    # loop through length of generated text, sampling along the way\n",
    "    for i in range(length-plen):\n",
    "        p = root.eval(arguments)\n",
    "        idx = sample_word(p)\n",
    "        output.append(idx)\n",
    "\n",
    "        x = np.zeros((1, vocab_dim), dtype=np.float32)\n",
    "        x[0, idx] = 1\n",
    "        arguments = ([x], [False])\n",
    "\n",
    "    # return output\n",
    "    return ''.join([ix_to_char[c] for c in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_vocab(training_file):\n",
    "    \n",
    "    # load data\n",
    "    rel_path = training_file\n",
    "    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), rel_path)\n",
    "    data = open(path, \"r\").read()\n",
    "    chars = sorted(list(set(data)))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "    # write vocab for future use\n",
    "    with open(path + \".vocab\", \"w\") as ff:\n",
    "        for c in chars:\n",
    "            ff.write(\"%s\\n\" % c) if c != '\\n' else ff.write(\"\\n\")\n",
    "    \n",
    "    return data, char_to_ix, ix_to_char, data_size, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the model to train\n",
    "def create_model(output_dim):\n",
    "    \n",
    "    return Sequential([        \n",
    "        For(range(num_layers), lambda: \n",
    "                   Sequential([Stabilizer(), Recurrence(LSTM(hidden_dim), go_backwards=False)])),\n",
    "        Dense(output_dim)\n",
    "    ])\n",
    "\n",
    "# Model inputs\n",
    "def create_inputs(vocab_dim):\n",
    "    input_seq_axis = Axis('inputAxis')\n",
    "    input_sequence = sequence.input_variable(shape=vocab_dim, sequence_axis=input_seq_axis)\n",
    "    label_sequence = sequence.input_variable(shape=vocab_dim, sequence_axis=input_seq_axis)\n",
    "    \n",
    "    return input_sequence, label_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and trains a character-level language model\n",
    "def train_lm(training_file, epochs, max_num_minibatches):\n",
    "\n",
    "    # load the data and vocab\n",
    "    data, char_to_ix, ix_to_char, data_size, vocab_dim = load_data_and_vocab(training_file)\n",
    "\n",
    "    # Model the source and target inputs to the model\n",
    "    input_sequence, label_sequence = create_inputs(vocab_dim)\n",
    "\n",
    "    # create the model\n",
    "    model = create_model(vocab_dim)\n",
    "    \n",
    "    # and apply it to the input sequence    \n",
    "    z = model(input_sequence)\n",
    "\n",
    "    # setup the criterions (loss and metric)\n",
    "    ce = cross_entropy_with_softmax(z, label_sequence)\n",
    "    errs = classification_error(z, label_sequence)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    momentum_time_constant = momentum_as_time_constant_schedule(1100)\n",
    "    clipping_threshold_per_sample = 5.0\n",
    "    gradient_clipping_with_truncation = True\n",
    "    learner = momentum_sgd(z.parameters, lr_per_sample, momentum_time_constant,\n",
    "                           gradient_clipping_threshold_per_sample=clipping_threshold_per_sample,\n",
    "                           gradient_clipping_with_truncation=gradient_clipping_with_truncation)\n",
    "    progress_printer = ProgressPrinter(freq=100, tag='Training')\n",
    "    trainer = Trainer(z, (ce, errs), learner, progress_printer)\n",
    "\n",
    "    sample_freq = 1000\n",
    "    minibatches_per_epoch = min(data_size // minibatch_size, max_num_minibatches // epochs)\n",
    "\n",
    "    # print out some useful training information\n",
    "    log_number_of_parameters(z)\n",
    "    print (\"Running %d epochs with %d minibatches per epoch\" % (epochs, minibatches_per_epoch))\n",
    "    print()\n",
    "\n",
    "    for e in range(0, epochs):\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        # If it's the start of the data, we specify that we are looking at a new sequence (True)\n",
    "        mask = [True]\n",
    "        for b in range(0, minibatches_per_epoch):\n",
    "            # get the data            \n",
    "            features, labels = get_data(b, minibatch_size, data, char_to_ix, vocab_dim)\n",
    "            arguments = ({input_sequence : features, label_sequence : labels}, mask)\n",
    "            mask = [False] \n",
    "            trainer.train_minibatch(arguments)\n",
    "\n",
    "            global_minibatch = e*minibatches_per_epoch + b\n",
    "            if global_minibatch % sample_freq == 0:\n",
    "                print(sample(z, ix_to_char, vocab_dim, char_to_ix))\n",
    "\n",
    "        model_filename = \"models/shakespeare_epoch%d.dnn\" % (e+1)\n",
    "        z.save(model_filename)\n",
    "        print(\"Saved model to '%s'\" % model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample(model_filename, vocab_filename, prime_text='', use_hardmax=False, length=1000, temperature=1.0):\n",
    "    \n",
    "    # load the model\n",
    "    model = load_model(model_filename)\n",
    "    \n",
    "    # load the vocab\n",
    "    vocab_filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), vocab_filename)\n",
    "    chars = [c[0] for c in open(vocab_filepath).readlines()]\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "    return sample(model, ix_to_char, len(chars), char_to_ix, prime_text=prime_text, use_hardmax=use_hardmax, length=length, temperature=temperature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_char_rnn(epochs=50, max_num_minibatches=sys.maxsize):\n",
    "    # train the LM \n",
    "    train_lm(\"data/tinyshakespeare.txt\", epochs, max_num_minibatches)\n",
    "\n",
    "    # load and sample\n",
    "    text = \"T\"\n",
    "    return load_and_sample(\"models/shakespeare_epoch%d.dnn\" % (epochs), \"data/tinyshakespeare.txt.vocab\", prime_text=text, use_hardmax=False, length=100, temperature=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = train_and_eval_char_rnn()\n",
    "ff = open('output.txt', 'w', encoding='utf-8')\n",
    "ff.write(output)\n",
    "ff.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cntk-py35",
   "language": "python",
   "name": "cntk-py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
